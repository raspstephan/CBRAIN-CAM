{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tgb - 4/18/2019 \n",
    "- We will train unconstrained (U), loss-constrained (L) and architecture-constrained (A) neural networks trained on the 8-column +0K experiment and validated on the same experiment. The loss-constrained networks will have varying architectures to see how MSE and energy conservation performances vary with the importance given to each in the loss function.  \n",
    "  \n",
    "Notebook 009 follows the notebook 005 that predicts:\n",
    "***\n",
    "[PHQ, PHCLDLIQ, PHCLDICE, TPHYSTND, QRL, QRS, DTVKE, FSNT, FSNS, FLNT, FLNS, PRECT, PRECTEND, PRECST, PRECSTEN] as a function of:  \n",
    "[QBP, QCBP, QIBP, TBP, VBP, Qdt_adiabatic, QCdt_adiabatic, QIdt_adiabatic, Tdt_adiabatic, Vdt_adiabatic, PS, SOLIN, SHFLX, LHFLX] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Load modules and create training/validation data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (imports.py, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/oasis/scratch/comet/tbeucler/temp_project/CBRAIN-CAM/notebooks/tbeucler_devlog/cbrain/imports.py\"\u001b[0;36m, line \u001b[0;32m24\u001b[0m\n\u001b[0;31m    sys.path.append(f'{base_dir}keras_network/')\u001b[0m\n\u001b[0m                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from cbrain.imports import *\n",
    "from cbrain.data_generator import *\n",
    "from cbrain.cam_constants import *\n",
    "from cbrain.losses import *\n",
    "from cbrain.utils import limit_mem\n",
    "from cbrain.layers import *\n",
    "import tensorflow as tf\n",
    "import tensorflow.math as tfm\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from cbrain.model_diagnostics import ModelDiagnostics\n",
    "# Otherwise tensorflow will use ALL your GPU RAM for no reason\n",
    "limit_mem()\n",
    "TRAINDIR = '/local/Tom.Beucler/SPCAM_PHYS/'\n",
    "DATADIR = '/project/meteo/w2w/A6/S.Rasp/SP-CAM/fluxbypass_aqua/'\n",
    "PREFIX = '8col009_01_'\n",
    "#PREFIXDS = '8col009_ds1_'\n",
    "PREFIXDS = PREFIX\n",
    "%cd /filer/z-sv-pool12c/t/Tom.Beucler/SPCAM/CBRAIN-CAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tgb - 4/18/2019 - Used preprocessed data calculated in notebook 009 @ https://github.com/tbeucler/CBRAIN-CAM/blob/master/notebooks/tbeucler_devlog/009_Generalization_Climate_Change_8col.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_dict = load_pickle('./nn_config/scale_dicts/009_Wm2_scaling.pkl')\n",
    "in_vars = load_pickle('./nn_config/scale_dicts/009_Wm2_in_vars.pkl')\n",
    "out_vars = load_pickle('./nn_config/scale_dicts/009_Wm2_out_vars.pkl')\n",
    "dP = load_pickle('./nn_config/scale_dicts/009_Wm2_dP.pkl')\n",
    "\n",
    "train_gen = DataGenerator(\n",
    "    data_fn = TRAINDIR+PREFIXDS+'train_shuffle.nc',\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = TRAINDIR+PREFIX+'norm.nc',\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=True\n",
    ")\n",
    "valid_gen = DataGenerator(\n",
    "    data_fn = TRAINDIR+PREFIX+'valid.nc',\n",
    "    input_vars = in_vars,\n",
    "    output_vars = out_vars,\n",
    "    norm_fn = TRAINDIR+PREFIX+'norm.nc',\n",
    "    input_transform = ('mean', 'maxrs'),\n",
    "    output_transform = scale_dict,\n",
    "    batch_size=1024,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Build neural networks in a loop\n",
    "tgb - 4/18/2019 - https://stackoverflow.com/questions/52320059/creating-a-new-sequential-model-inside-a-for-loop-using-keras  \n",
    "tgb - 4/24/2019 - Edited for data-scarce network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The Session graph is empty.  Add operations to the graph before calling run().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-488b8c51de0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 5) Save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTRAINDIR\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'HDF5_DATA/NNL'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'DS1_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NN saved in '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m   1332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m     \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/saving.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mmodel_weights_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mmodel_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0msave_weights_to_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_weights_group\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minclude_optimizer\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/saving.py\u001b[0m in \u001b[0;36msave_weights_to_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m    740\u001b[0m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m     \u001b[0msymbolic_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m     \u001b[0mweight_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    743\u001b[0m     \u001b[0mweight_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mbatch_get_value\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m   2817\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot get value inside Tensorflow graph function.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2818\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2819\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2820\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2821\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1075\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to use a closed Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1077\u001b[0;31m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n\u001b[0m\u001b[1;32m   1078\u001b[0m                          'graph before calling run().')\n\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The Session graph is empty.  Add operations to the graph before calling run()."
     ]
    }
   ],
   "source": [
    "hsav = {}\n",
    "hsav[ep] = hist.history\n",
    "\n",
    "# 5) Save model\n",
    "path = TRAINDIR+'HDF5_DATA/NNL'+str(alpha)+'DS1_'+str(ep)+'.h5'\n",
    "NN.save(path)\n",
    "print('NN saved in ',path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha =  0  and NN is  {}\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 304)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               156160    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 218)               111834    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 218)               0         \n",
      "=================================================================\n",
      "Total params: 1,318,618\n",
      "Trainable params: 1,318,618\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "NN is  None\n",
      "Epoch 1/100\n",
      "376/376 [==============================] - 4s 11ms/step - loss: 767.6149 - mean_squared_error: 767.6146\n",
      "Epoch 2/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 416.3703 - mean_squared_error: 416.3700\n",
      "Epoch 3/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 369.7510 - mean_squared_error: 369.7509\n",
      "Epoch 4/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 342.9424 - mean_squared_error: 342.9425\n",
      "Epoch 5/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 325.0113 - mean_squared_error: 325.0113\n",
      "Epoch 6/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 311.6880 - mean_squared_error: 311.6880\n",
      "Epoch 7/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 302.0209 - mean_squared_error: 302.0208\n",
      "Epoch 8/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 292.4785 - mean_squared_error: 292.4785\n",
      "Epoch 9/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 285.0378 - mean_squared_error: 285.0378\n",
      "Epoch 10/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 278.2165 - mean_squared_error: 278.2166\n",
      "Epoch 11/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 272.2543 - mean_squared_error: 272.2542\n",
      "Epoch 12/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 267.5413 - mean_squared_error: 267.5413\n",
      "Epoch 13/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 263.1735 - mean_squared_error: 263.1736\n",
      "Epoch 14/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 258.5639 - mean_squared_error: 258.5638\n",
      "Epoch 15/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 255.1182 - mean_squared_error: 255.1182\n",
      "Epoch 16/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 252.1296 - mean_squared_error: 252.1296\n",
      "Epoch 17/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 249.1452 - mean_squared_error: 249.1452\n",
      "Epoch 18/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 246.5305 - mean_squared_error: 246.5307\n",
      "Epoch 19/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 243.6824 - mean_squared_error: 243.6824\n",
      "Epoch 20/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 241.7404 - mean_squared_error: 241.7405\n",
      "Epoch 21/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 239.2796 - mean_squared_error: 239.2795\n",
      "Epoch 22/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 236.7178 - mean_squared_error: 236.7178\n",
      "Epoch 23/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 235.1199 - mean_squared_error: 235.1198\n",
      "Epoch 24/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 233.3892 - mean_squared_error: 233.3891\n",
      "Epoch 25/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 230.9654 - mean_squared_error: 230.9654\n",
      "Epoch 26/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 229.5804 - mean_squared_error: 229.5805\n",
      "Epoch 27/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 227.7331 - mean_squared_error: 227.7329\n",
      "Epoch 28/100\n",
      "376/376 [==============================] - 4s 11ms/step - loss: 226.3296 - mean_squared_error: 226.3297\n",
      "Epoch 29/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 224.5192 - mean_squared_error: 224.5193\n",
      "Epoch 30/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 223.4058 - mean_squared_error: 223.4057\n",
      "Epoch 31/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 221.2703 - mean_squared_error: 221.2702\n",
      "Epoch 32/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 220.2382 - mean_squared_error: 220.2382\n",
      "Epoch 33/100\n",
      "376/376 [==============================] - 4s 11ms/step - loss: 218.9509 - mean_squared_error: 218.9510\n",
      "Epoch 34/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 217.6224 - mean_squared_error: 217.6223\n",
      "Epoch 35/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 216.5969 - mean_squared_error: 216.5970\n",
      "Epoch 36/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 215.0754 - mean_squared_error: 215.0754\n",
      "Epoch 37/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 214.3012 - mean_squared_error: 214.3012\n",
      "Epoch 38/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 212.9014 - mean_squared_error: 212.9014\n",
      "Epoch 39/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 211.8288 - mean_squared_error: 211.8287\n",
      "Epoch 40/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 211.1599 - mean_squared_error: 211.1600\n",
      "Epoch 41/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 209.7029 - mean_squared_error: 209.7029\n",
      "Epoch 42/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 208.9745 - mean_squared_error: 208.9744\n",
      "Epoch 43/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 207.1299 - mean_squared_error: 207.1298\n",
      "Epoch 44/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 206.4022 - mean_squared_error: 206.4022\n",
      "Epoch 45/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 205.6787 - mean_squared_error: 205.6787\n",
      "Epoch 46/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 204.5129 - mean_squared_error: 204.5129\n",
      "Epoch 47/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 203.3414 - mean_squared_error: 203.3415\n",
      "Epoch 48/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 202.3025 - mean_squared_error: 202.3025\n",
      "Epoch 49/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 201.6082 - mean_squared_error: 201.6082\n",
      "Epoch 50/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 200.4811 - mean_squared_error: 200.4811\n",
      "Epoch 51/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 199.2641 - mean_squared_error: 199.2641\n",
      "Epoch 52/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 198.6083 - mean_squared_error: 198.6083\n",
      "Epoch 53/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 197.6177 - mean_squared_error: 197.6177\n",
      "Epoch 54/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 196.5293 - mean_squared_error: 196.5293\n",
      "Epoch 55/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 196.1616 - mean_squared_error: 196.1617\n",
      "Epoch 56/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 194.7804 - mean_squared_error: 194.7803\n",
      "Epoch 57/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 194.1584 - mean_squared_error: 194.1584\n",
      "Epoch 58/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 193.1040 - mean_squared_error: 193.1040\n",
      "Epoch 59/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 191.8383 - mean_squared_error: 191.8382\n",
      "Epoch 60/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 191.4363 - mean_squared_error: 191.4363\n",
      "Epoch 61/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 190.3009 - mean_squared_error: 190.3009\n",
      "Epoch 62/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 189.4859 - mean_squared_error: 189.4858\n",
      "Epoch 63/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 188.8298 - mean_squared_error: 188.8299\n",
      "Epoch 64/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 188.2802 - mean_squared_error: 188.2802\n",
      "Epoch 65/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 187.1132 - mean_squared_error: 187.1131\n",
      "Epoch 66/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 186.1699 - mean_squared_error: 186.1699\n",
      "Epoch 67/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 185.1556 - mean_squared_error: 185.1555\n",
      "Epoch 68/100\n",
      "376/376 [==============================] - 4s 11ms/step - loss: 184.8775 - mean_squared_error: 184.8775\n",
      "Epoch 69/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 184.1881 - mean_squared_error: 184.1880\n",
      "Epoch 70/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 183.5268 - mean_squared_error: 183.5268\n",
      "Epoch 71/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 182.1862 - mean_squared_error: 182.1862\n",
      "Epoch 72/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 181.6780 - mean_squared_error: 181.6780\n",
      "Epoch 73/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 180.9516 - mean_squared_error: 180.9516\n",
      "Epoch 74/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 180.1638 - mean_squared_error: 180.1639\n",
      "Epoch 75/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 179.1491 - mean_squared_error: 179.1491\n",
      "Epoch 76/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 178.7298 - mean_squared_error: 178.7298\n",
      "Epoch 77/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 177.7100 - mean_squared_error: 177.7100\n",
      "Epoch 78/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 176.9539 - mean_squared_error: 176.9539\n",
      "Epoch 79/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 175.9157 - mean_squared_error: 175.9157\n",
      "Epoch 80/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 175.4507 - mean_squared_error: 175.4507\n",
      "Epoch 81/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 174.3735 - mean_squared_error: 174.3735\n",
      "Epoch 82/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 173.7643 - mean_squared_error: 173.7643\n",
      "Epoch 83/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 173.1965 - mean_squared_error: 173.1965\n",
      "Epoch 84/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 172.9366 - mean_squared_error: 172.9367\n",
      "Epoch 85/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 171.4853 - mean_squared_error: 171.4853\n",
      "Epoch 86/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 171.2050 - mean_squared_error: 171.2051\n",
      "Epoch 87/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 170.4621 - mean_squared_error: 170.4621\n",
      "Epoch 88/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 169.6351 - mean_squared_error: 169.6352\n",
      "Epoch 89/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 168.8800 - mean_squared_error: 168.8801\n",
      "Epoch 90/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 168.3840 - mean_squared_error: 168.3839\n",
      "Epoch 91/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 167.2402 - mean_squared_error: 167.2402\n",
      "Epoch 92/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 166.9400 - mean_squared_error: 166.9401\n",
      "Epoch 93/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 165.9973 - mean_squared_error: 165.9973\n",
      "Epoch 94/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 165.7263 - mean_squared_error: 165.7263\n",
      "Epoch 95/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 164.3677 - mean_squared_error: 164.3677\n",
      "Epoch 96/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 164.2158 - mean_squared_error: 164.2158\n",
      "Epoch 97/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 163.2885 - mean_squared_error: 163.2885\n",
      "Epoch 98/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 162.8532 - mean_squared_error: 162.8532\n",
      "Epoch 99/100\n",
      "376/376 [==============================] - 4s 10ms/step - loss: 161.9904 - mean_squared_error: 161.9903\n",
      "Epoch 100/100\n",
      "376/376 [==============================] - 4s 9ms/step - loss: 161.5184 - mean_squared_error: 161.5184\n",
      "NN saved in  /local/Tom.Beucler/SPCAM_PHYS/HDF5_DATA/NNL0DS1.h5\n"
     ]
    }
   ],
   "source": [
    "#alpha_array = [0,0.01,0.25,0.5,0.75,0.99,1] # Loop over weight given to MSE and conservation constraints\n",
    "alpha_array = [0]\n",
    "Nep = 100; hsav = {};\n",
    "\n",
    "for alpha in alpha_array:\n",
    "    NN = {}\n",
    "    print('alpha = ',str(alpha),' and NN is ',NN)\n",
    "    graph = tf.Graph()\n",
    "    with tf.Session(graph=graph):\n",
    "        \n",
    "        # 1) Create model\n",
    "        # Unconstrained model with 5 dense layers (Notebook 009)\n",
    "        inpU = Input(shape=(304,))\n",
    "        densout = Dense(512, activation='linear')(inpU)\n",
    "        densout = LeakyReLU(alpha=0.3)(densout)\n",
    "        for i in range (4):\n",
    "            densout = Dense(512, activation='linear')(densout)\n",
    "            densout = LeakyReLU(alpha=0.3)(densout)\n",
    "        densout = Dense(218, activation='linear')(densout)\n",
    "        out_layer = LeakyReLU(alpha=0.3)(densout)\n",
    "        NN = tf.keras.models.Model(inpU, out_layer)\n",
    "        print('NN is ',NN.summary())\n",
    "        \n",
    "        # 2) Define loss\n",
    "        al = alpha/4 # Weight given to each residual\n",
    "        Loss = WeakLoss(inpU, inp_div=train_gen.input_transform.div,\n",
    "                            inp_sub=train_gen.input_transform.sub,\n",
    "                            norm_q=scale_dict['PHQ'],\n",
    "                            hyai=hyai, hybi=hybi, name='loss',\n",
    "                            alpha_mass=al, alpha_ent=al,\n",
    "                            alpha_lw=al, alpha_sw=al)\n",
    "        \n",
    "        # 3) Compile model\n",
    "        NN.compile(tf.keras.optimizers.RMSprop(), loss=Loss, metrics=[mse])\n",
    "        \n",
    "        # 4) Train model\n",
    "        # Save history following https://stackoverflow.com/questions/49969006/save-and-load-keras-callbacks-history\n",
    "#         for ep in range(20):\n",
    "        hist = NN.fit_generator(train_gen, epochs=Nep)\n",
    "        hsav = hist.history\n",
    "\n",
    "        # 5) Save model\n",
    "        path = TRAINDIR+'HDF5_DATA/NNL'+str(alpha)+'DS1.h5'\n",
    "        NN.save(path)\n",
    "        print('NN saved in ',path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tgb - 5/1/2019 - Modified to create NNA network with 0.01=alpha loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/t/Tom.Beucler/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 304)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          156160      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 512)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          262656      leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 512)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          262656      leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 512)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 512)          262656      leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 512)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 512)          262656      leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 512)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 214)          109782      leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 214)          0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sur_rad_layer (SurRadLayer)     (None, 216)          0           input_1[0][0]                    \n",
      "                                                                 leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mass_cons_layer (MassConsLayer) (None, 217)          0           input_1[0][0]                    \n",
      "                                                                 sur_rad_layer[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ent_cons_layer (EntConsLayer)   (None, 218)          0           input_1[0][0]                    \n",
      "                                                                 mass_cons_layer[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 1,316,566\n",
      "Trainable params: 1,316,566\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /home/t/Tom.Beucler/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "41376/41376 [==============================] - 513s 12ms/step - loss: 199.3468 - mean_squared_error: 201.3604\n",
      "41376/41376 [==============================] - 1292s 31ms/step - loss: 218.3491 - mean_squared_error: 220.5551 - val_loss: 199.3468 - val_mean_squared_error: 201.3604\n",
      "Epoch 2/20\n",
      "41376/41376 [==============================] - 506s 12ms/step - loss: 190.4319 - mean_squared_error: 192.3550\n",
      "41376/41376 [==============================] - 1270s 31ms/step - loss: 178.1832 - mean_squared_error: 179.9833 - val_loss: 190.4319 - val_mean_squared_error: 192.3550\n",
      "Epoch 3/20\n",
      "41376/41376 [==============================] - 493s 12ms/step - loss: 169.3752 - mean_squared_error: 171.0855\n",
      "41376/41376 [==============================] - 1258s 30ms/step - loss: 171.9679 - mean_squared_error: 173.7045 - val_loss: 169.3752 - val_mean_squared_error: 171.0855\n",
      "Epoch 4/20\n",
      "41376/41376 [==============================] - 479s 12ms/step - loss: 167.0305 - mean_squared_error: 168.7169\n",
      "41376/41376 [==============================] - 1245s 30ms/step - loss: 168.7078 - mean_squared_error: 170.4117 - val_loss: 167.0305 - val_mean_squared_error: 168.7169\n",
      "Epoch 5/20\n",
      "41376/41376 [==============================] - 510s 12ms/step - loss: 166.2582 - mean_squared_error: 167.9375\n",
      "41376/41376 [==============================] - 1274s 31ms/step - loss: 166.6478 - mean_squared_error: 168.3313 - val_loss: 166.2582 - val_mean_squared_error: 167.9375\n",
      "Epoch 6/20\n",
      "41376/41376 [==============================] - 243s 6ms/step - loss: 162.3131 - mean_squared_error: 163.9527\n",
      "41376/41376 [==============================] - 932s 23ms/step - loss: 165.0956 - mean_squared_error: 166.7633 - val_loss: 162.3131 - val_mean_squared_error: 163.9527\n",
      "Epoch 7/20\n",
      " 4791/41376 [==>...........................] - ETA: 6:25 - loss: 164.3403 - mean_squared_error: 166.0005"
     ]
    }
   ],
   "source": [
    "Nep = 20\n",
    "# Repeat for architecture-constrained network\n",
    "graph = tf.Graph()\n",
    "with tf.Session(graph=graph):\n",
    "\n",
    "    # 1) Create model\n",
    "    # Unconstrained model with 5 dense layers (Notebook 009)\n",
    "    inpC = Input(shape=(304,))\n",
    "    densout = Dense(512, activation='linear')(inpC)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "    for i in range (4):\n",
    "        densout = Dense(512, activation='linear')(densout)\n",
    "        densout = LeakyReLU(alpha=0.3)(densout)\n",
    "    densout = Dense(214, activation='linear')(densout)\n",
    "    densout = LeakyReLU(alpha=0.3)(densout)\n",
    "    surfout = SurRadLayer(\n",
    "        inp_div=train_gen.input_transform.div,\n",
    "        inp_sub=train_gen.input_transform.sub,\n",
    "        norm_q=scale_dict['PHQ'],\n",
    "        hyai=hyai, hybi=hybi\n",
    "    )([inpC, densout])\n",
    "    massout = MassConsLayer(\n",
    "        inp_div=train_gen.input_transform.div,\n",
    "        inp_sub=train_gen.input_transform.sub,\n",
    "        norm_q=scale_dict['PHQ'],\n",
    "        hyai=hyai, hybi=hybi\n",
    "    )([inpC, surfout])\n",
    "    enthout = EntConsLayer(\n",
    "        inp_div=train_gen.input_transform.div,\n",
    "        inp_sub=train_gen.input_transform.sub,\n",
    "        norm_q=scale_dict['PHQ'],\n",
    "        hyai=hyai, hybi=hybi\n",
    "    )([inpC, massout])\n",
    "    NNA = tf.keras.models.Model(inpC, enthout)\n",
    "    print(NNA.summary())\n",
    "    \n",
    "    al = 0.01/4\n",
    "    Loss = WeakLoss(inpC, inp_div=train_gen.input_transform.div,\n",
    "                            inp_sub=train_gen.input_transform.sub,\n",
    "                            norm_q=scale_dict['PHQ'],\n",
    "                            hyai=hyai, hybi=hybi, name='loss',\n",
    "                            alpha_mass=al, alpha_ent=al,\n",
    "                            alpha_lw=al, alpha_sw=al)\n",
    "    \n",
    "    # 2) Compile model\n",
    "    NNA.compile(tf.keras.optimizers.RMSprop(), loss=Loss, metrics=[mse])\n",
    "\n",
    "    # 3) Train model\n",
    "    NNA.fit_generator(train_gen, epochs=Nep, validation_data=valid_gen)\n",
    "\n",
    "    # 4) Save model\n",
    "    path = TRAINDIR+'HDF5_DATA/NNA0.01.h5'\n",
    "    NNA.save(path)\n",
    "    print('NN saved in ',path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tgb - 4/23/2019 - Repeat unconstrained and architecture-constrained examples in multiple linear regression mode to compare performances.  \n",
    "To make the models linear, simply eliminate the leaky reLU layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha =  0  and NN is  {}\n",
      "WARNING:tensorflow:From /home/t/Tom.Beucler/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 304)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               156160    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 218)               111834    \n",
      "=================================================================\n",
      "Total params: 1,318,618\n",
      "Trainable params: 1,318,618\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "NN is  None\n",
      "WARNING:tensorflow:From /home/t/Tom.Beucler/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "41376/41376 [==============================] - 413s 10ms/step - loss: 306.8861 - mean_squared_error: 306.8857\n",
      "41376/41376 [==============================] - 1564s 38ms/step - loss: 315.1179 - mean_squared_error: 315.1190 - val_loss: 306.8861 - val_mean_squared_error: 306.8857\n",
      "Epoch 2/20\n",
      "41376/41376 [==============================] - 282s 7ms/step - loss: 298.2975 - mean_squared_error: 298.2973\n",
      "41376/41376 [==============================] - 1515s 37ms/step - loss: 296.0228 - mean_squared_error: 296.0225 - val_loss: 298.2975 - val_mean_squared_error: 298.2973\n",
      "Epoch 3/20\n",
      "41376/41376 [==============================] - 273s 7ms/step - loss: 298.4825 - mean_squared_error: 298.4820\n",
      "41376/41376 [==============================] - 698s 17ms/step - loss: 293.7869 - mean_squared_error: 293.7867 - val_loss: 298.4825 - val_mean_squared_error: 298.4820\n",
      "Epoch 4/20\n",
      "41376/41376 [==============================] - 275s 7ms/step - loss: 297.7656 - mean_squared_error: 297.7640\n",
      "41376/41376 [==============================] - 705s 17ms/step - loss: 292.7091 - mean_squared_error: 292.7096 - val_loss: 297.7656 - val_mean_squared_error: 297.7640\n",
      "Epoch 5/20\n",
      "41376/41376 [==============================] - 274s 7ms/step - loss: 295.8810 - mean_squared_error: 295.8824\n",
      "41376/41376 [==============================] - 703s 17ms/step - loss: 292.0568 - mean_squared_error: 292.0567 - val_loss: 295.8810 - val_mean_squared_error: 295.8824\n",
      "Epoch 6/20\n",
      "41376/41376 [==============================] - 276s 7ms/step - loss: 295.5634 - mean_squared_error: 295.5620\n",
      "41376/41376 [==============================] - 704s 17ms/step - loss: 291.6121 - mean_squared_error: 291.6128 - val_loss: 295.5634 - val_mean_squared_error: 295.5620\n",
      "Epoch 7/20\n",
      "41376/41376 [==============================] - 274s 7ms/step - loss: 294.6550 - mean_squared_error: 294.6543\n",
      "41376/41376 [==============================] - 703s 17ms/step - loss: 291.2915 - mean_squared_error: 291.2912 - val_loss: 294.6550 - val_mean_squared_error: 294.6543\n",
      "Epoch 8/20\n",
      "41376/41376 [==============================] - 275s 7ms/step - loss: 294.7396 - mean_squared_error: 294.7395\n",
      "41376/41376 [==============================] - 703s 17ms/step - loss: 291.0312 - mean_squared_error: 291.0316 - val_loss: 294.7396 - val_mean_squared_error: 294.7395\n",
      "Epoch 9/20\n",
      "41376/41376 [==============================] - 275s 7ms/step - loss: 295.2952 - mean_squared_error: 295.2942\n",
      "41376/41376 [==============================] - 705s 17ms/step - loss: 290.8330 - mean_squared_error: 290.8333 - val_loss: 295.2952 - val_mean_squared_error: 295.2942\n",
      "Epoch 10/20\n",
      "41376/41376 [==============================] - 282s 7ms/step - loss: 295.3156 - mean_squared_error: 295.3157\n",
      "41376/41376 [==============================] - 721s 17ms/step - loss: 290.6617 - mean_squared_error: 290.6609 - val_loss: 295.3156 - val_mean_squared_error: 295.3157\n",
      "Epoch 11/20\n",
      "41376/41376 [==============================] - 284s 7ms/step - loss: 297.6702 - mean_squared_error: 297.6704\n",
      "41376/41376 [==============================] - 721s 17ms/step - loss: 290.5128 - mean_squared_error: 290.5121 - val_loss: 297.6702 - val_mean_squared_error: 297.6704\n",
      "Epoch 12/20\n",
      "41376/41376 [==============================] - 282s 7ms/step - loss: 295.3276 - mean_squared_error: 295.3268\n",
      "41376/41376 [==============================] - 723s 17ms/step - loss: 290.3858 - mean_squared_error: 290.3867 - val_loss: 295.3276 - val_mean_squared_error: 295.3268\n",
      "Epoch 13/20\n",
      "41376/41376 [==============================] - 284s 7ms/step - loss: 295.1985 - mean_squared_error: 295.1980\n",
      "41376/41376 [==============================] - 724s 17ms/step - loss: 290.2757 - mean_squared_error: 290.2770 - val_loss: 295.1985 - val_mean_squared_error: 295.1980\n",
      "Epoch 14/20\n",
      "41376/41376 [==============================] - 280s 7ms/step - loss: 293.9674 - mean_squared_error: 293.9677\n",
      "41376/41376 [==============================] - 720s 17ms/step - loss: 290.1853 - mean_squared_error: 290.1844 - val_loss: 293.9674 - val_mean_squared_error: 293.9677\n",
      "Epoch 15/20\n",
      "41376/41376 [==============================] - 279s 7ms/step - loss: 294.0989 - mean_squared_error: 294.0993\n",
      "41376/41376 [==============================] - 716s 17ms/step - loss: 290.1036 - mean_squared_error: 290.1035 - val_loss: 294.0989 - val_mean_squared_error: 294.0993\n",
      "Epoch 16/20\n",
      "41376/41376 [==============================] - 280s 7ms/step - loss: 293.4726 - mean_squared_error: 293.4739\n",
      "41376/41376 [==============================] - 717s 17ms/step - loss: 290.0251 - mean_squared_error: 290.0251 - val_loss: 293.4726 - val_mean_squared_error: 293.4739\n",
      "Epoch 17/20\n",
      "41376/41376 [==============================] - 278s 7ms/step - loss: 294.0073 - mean_squared_error: 294.0067\n",
      "41376/41376 [==============================] - 714s 17ms/step - loss: 289.9634 - mean_squared_error: 289.9648 - val_loss: 294.0073 - val_mean_squared_error: 294.0067\n",
      "Epoch 18/20\n",
      "41376/41376 [==============================] - 281s 7ms/step - loss: 293.4191 - mean_squared_error: 293.4185\n",
      "41376/41376 [==============================] - 718s 17ms/step - loss: 289.8969 - mean_squared_error: 289.8969 - val_loss: 293.4191 - val_mean_squared_error: 293.4185\n",
      "Epoch 19/20\n",
      "41376/41376 [==============================] - 279s 7ms/step - loss: 294.1425 - mean_squared_error: 294.1431\n",
      "41376/41376 [==============================] - 713s 17ms/step - loss: 289.8445 - mean_squared_error: 289.8451 - val_loss: 294.1425 - val_mean_squared_error: 294.1431\n",
      "Epoch 20/20\n",
      "41376/41376 [==============================] - 276s 7ms/step - loss: 295.2750 - mean_squared_error: 295.2746\n",
      "41376/41376 [==============================] - 712s 17ms/step - loss: 289.7875 - mean_squared_error: 289.7863 - val_loss: 295.2750 - val_mean_squared_error: 295.2746\n",
      "MLR saved in  /local/Tom.Beucler/SPCAM_PHYS/HDF5_DATA/MLRL0.h5\n"
     ]
    }
   ],
   "source": [
    "Nep = 20\n",
    "alpha = 0\n",
    "\n",
    "NN = {}\n",
    "print('alpha = ',str(alpha),' and NN is ',NN)\n",
    "graph = tf.Graph()\n",
    "with tf.Session(graph=graph):\n",
    "\n",
    "    # 1) Create model\n",
    "    # Unconstrained model with 5 dense layers (Notebook 009)\n",
    "    inpU = Input(shape=(304,))\n",
    "    densout = Dense(512, activation='linear')(inpU)\n",
    "    for i in range (4):\n",
    "        densout = Dense(512, activation='linear')(densout)\n",
    "    out_layer = Dense(218, activation='linear')(densout)\n",
    "    NN = tf.keras.models.Model(inpU, out_layer)\n",
    "    print('NN is ',NN.summary())\n",
    "\n",
    "    # 2) Define loss\n",
    "    al = alpha/4 # Weight given to each residual\n",
    "    Loss = WeakLoss(inpU, inp_div=train_gen.input_transform.div,\n",
    "                        inp_sub=train_gen.input_transform.sub,\n",
    "                        norm_q=scale_dict['PHQ'],\n",
    "                        hyai=hyai, hybi=hybi, name='loss',\n",
    "                        alpha_mass=al, alpha_ent=al,\n",
    "                        alpha_lw=al, alpha_sw=al)\n",
    "\n",
    "    # 3) Compile model\n",
    "    NN.compile(tf.keras.optimizers.RMSprop(), loss=Loss, metrics=[mse])\n",
    "\n",
    "    # 4) Train model\n",
    "    NN.fit_generator(train_gen, epochs=Nep, validation_data=valid_gen)\n",
    "\n",
    "    # 5) Save model\n",
    "    path = TRAINDIR+'HDF5_DATA/MLRL'+str(alpha)+'.h5'\n",
    "    NN.save(path)\n",
    "    print('MLR saved in ',path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tgb - 4/23/2019 - Repeat the operation of linearing the neural network in the architecture-constrained case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 304)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          156160      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          262656      dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          262656      dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 512)          262656      dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 512)          262656      dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 214)          109782      dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sur_rad_layer (SurRadLayer)     (None, 216)          0           input_1[0][0]                    \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mass_cons_layer (MassConsLayer) (None, 217)          0           input_1[0][0]                    \n",
      "                                                                 sur_rad_layer[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "ent_cons_layer (EntConsLayer)   (None, 218)          0           input_1[0][0]                    \n",
      "                                                                 mass_cons_layer[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 1,316,566\n",
      "Trainable params: 1,316,566\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "41376/41376 [==============================] - 283s 7ms/step - loss: 303.3258 - mean_squared_error: 303.3254\n",
      "41376/41376 [==============================] - 728s 18ms/step - loss: 337.4312 - mean_squared_error: 337.4301 - val_loss: 303.3258 - val_mean_squared_error: 303.3254\n",
      "Epoch 2/20\n",
      "41376/41376 [==============================] - 284s 7ms/step - loss: 298.3324 - mean_squared_error: 298.3316\n",
      "41376/41376 [==============================] - 727s 18ms/step - loss: 300.2510 - mean_squared_error: 300.2511 - val_loss: 298.3324 - val_mean_squared_error: 298.3316\n",
      "Epoch 3/20\n",
      "41376/41376 [==============================] - 281s 7ms/step - loss: 301.3311 - mean_squared_error: 301.3307\n",
      "41376/41376 [==============================] - 719s 17ms/step - loss: 296.5336 - mean_squared_error: 296.5342 - val_loss: 301.3311 - val_mean_squared_error: 301.3307\n",
      "Epoch 4/20\n",
      "41376/41376 [==============================] - 280s 7ms/step - loss: 296.2898 - mean_squared_error: 296.2891\n",
      "41376/41376 [==============================] - 715s 17ms/step - loss: 294.9270 - mean_squared_error: 294.9271 - val_loss: 296.2898 - val_mean_squared_error: 296.2891\n",
      "Epoch 5/20\n",
      "41376/41376 [==============================] - 284s 7ms/step - loss: 296.5828 - mean_squared_error: 296.5843\n",
      "41376/41376 [==============================] - 720s 17ms/step - loss: 293.9492 - mean_squared_error: 293.9492 - val_loss: 296.5828 - val_mean_squared_error: 296.5843\n",
      "Epoch 6/20\n",
      "41376/41376 [==============================] - 285s 7ms/step - loss: 297.4616 - mean_squared_error: 297.4623\n",
      "41376/41376 [==============================] - 720s 17ms/step - loss: 293.2909 - mean_squared_error: 293.2905 - val_loss: 297.4616 - val_mean_squared_error: 297.4623\n",
      "Epoch 7/20\n",
      "41376/41376 [==============================] - 283s 7ms/step - loss: 297.4261 - mean_squared_error: 297.4275\n",
      "41376/41376 [==============================] - 718s 17ms/step - loss: 292.8904 - mean_squared_error: 292.8900 - val_loss: 297.4261 - val_mean_squared_error: 297.4275\n",
      "Epoch 8/20\n",
      "41376/41376 [==============================] - 284s 7ms/step - loss: 295.4028 - mean_squared_error: 295.4021\n",
      "41376/41376 [==============================] - 718s 17ms/step - loss: 292.5655 - mean_squared_error: 292.5649 - val_loss: 295.4028 - val_mean_squared_error: 295.4021\n",
      "Epoch 9/20\n",
      "41376/41376 [==============================] - 282s 7ms/step - loss: 295.6881 - mean_squared_error: 295.6886\n",
      "41376/41376 [==============================] - 716s 17ms/step - loss: 292.2359 - mean_squared_error: 292.2357 - val_loss: 295.6881 - val_mean_squared_error: 295.6886\n",
      "Epoch 10/20\n",
      "41376/41376 [==============================] - 283s 7ms/step - loss: 296.2090 - mean_squared_error: 296.2096\n",
      "41376/41376 [==============================] - 718s 17ms/step - loss: 292.0360 - mean_squared_error: 292.0353 - val_loss: 296.2090 - val_mean_squared_error: 296.2096\n",
      "Epoch 11/20\n",
      "41376/41376 [==============================] - 285s 7ms/step - loss: 296.6965 - mean_squared_error: 296.6950\n",
      "41376/41376 [==============================] - 720s 17ms/step - loss: 291.8235 - mean_squared_error: 291.8243 - val_loss: 296.6965 - val_mean_squared_error: 296.6950\n",
      "Epoch 12/20\n",
      "41376/41376 [==============================] - 285s 7ms/step - loss: 296.2663 - mean_squared_error: 296.2665\n",
      "41376/41376 [==============================] - 720s 17ms/step - loss: 291.6713 - mean_squared_error: 291.6725 - val_loss: 296.2663 - val_mean_squared_error: 296.2665\n",
      "Epoch 13/20\n",
      "41376/41376 [==============================] - 284s 7ms/step - loss: 295.6363 - mean_squared_error: 295.6366\n",
      "41376/41376 [==============================] - 718s 17ms/step - loss: 291.4953 - mean_squared_error: 291.4965 - val_loss: 295.6363 - val_mean_squared_error: 295.6366\n",
      "Epoch 14/20\n",
      "41376/41376 [==============================] - 284s 7ms/step - loss: 294.7006 - mean_squared_error: 294.7006\n",
      "41376/41376 [==============================] - 719s 17ms/step - loss: 291.3770 - mean_squared_error: 291.3772 - val_loss: 294.7006 - val_mean_squared_error: 294.7006\n",
      "Epoch 15/20\n",
      "41376/41376 [==============================] - 284s 7ms/step - loss: 295.7473 - mean_squared_error: 295.7476\n",
      "41376/41376 [==============================] - 718s 17ms/step - loss: 291.2771 - mean_squared_error: 291.2793 - val_loss: 295.7473 - val_mean_squared_error: 295.7476\n",
      "Epoch 16/20\n",
      "41376/41376 [==============================] - 286s 7ms/step - loss: 294.6094 - mean_squared_error: 294.6093\n",
      "41376/41376 [==============================] - 720s 17ms/step - loss: 291.1542 - mean_squared_error: 291.1544 - val_loss: 294.6094 - val_mean_squared_error: 294.6093\n",
      "Epoch 17/20\n",
      "41376/41376 [==============================] - 283s 7ms/step - loss: 294.0785 - mean_squared_error: 294.0768\n",
      "41376/41376 [==============================] - 718s 17ms/step - loss: 291.1443 - mean_squared_error: 291.1444 - val_loss: 294.0785 - val_mean_squared_error: 294.0768\n",
      "Epoch 18/20\n",
      "41376/41376 [==============================] - 286s 7ms/step - loss: 294.5590 - mean_squared_error: 294.5595\n",
      "41376/41376 [==============================] - 720s 17ms/step - loss: 291.0132 - mean_squared_error: 291.0136 - val_loss: 294.5590 - val_mean_squared_error: 294.5595\n",
      "Epoch 19/20\n",
      "41376/41376 [==============================] - 284s 7ms/step - loss: 294.3567 - mean_squared_error: 294.3560\n",
      "41376/41376 [==============================] - 715s 17ms/step - loss: 290.9539 - mean_squared_error: 290.9547 - val_loss: 294.3567 - val_mean_squared_error: 294.3560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20\n",
      "41376/41376 [==============================] - 284s 7ms/step - loss: 296.3175 - mean_squared_error: 296.3186\n",
      "41376/41376 [==============================] - 717s 17ms/step - loss: 290.9178 - mean_squared_error: 290.9177 - val_loss: 296.3175 - val_mean_squared_error: 296.3186\n",
      "MLR saved in  /local/Tom.Beucler/SPCAM_PHYS/HDF5_DATA/MLRA.h5\n"
     ]
    }
   ],
   "source": [
    "# Repeat for architecture-constrained network\n",
    "graph = tf.Graph()\n",
    "with tf.Session(graph=graph):\n",
    "\n",
    "    # 1) Create model\n",
    "    # Unconstrained model with 5 dense layers (Notebook 009)\n",
    "    inpC = Input(shape=(304,))\n",
    "    densout = Dense(512, activation='linear')(inpC)\n",
    "    for i in range (4):\n",
    "        densout = Dense(512, activation='linear')(densout)\n",
    "    densout = Dense(214, activation='linear')(densout)\n",
    "    surfout = SurRadLayer(\n",
    "        inp_div=train_gen.input_transform.div,\n",
    "        inp_sub=train_gen.input_transform.sub,\n",
    "        norm_q=scale_dict['PHQ'],\n",
    "        hyai=hyai, hybi=hybi\n",
    "    )([inpC, densout])\n",
    "    massout = MassConsLayer(\n",
    "        inp_div=train_gen.input_transform.div,\n",
    "        inp_sub=train_gen.input_transform.sub,\n",
    "        norm_q=scale_dict['PHQ'],\n",
    "        hyai=hyai, hybi=hybi\n",
    "    )([inpC, surfout])\n",
    "    enthout = EntConsLayer(\n",
    "        inp_div=train_gen.input_transform.div,\n",
    "        inp_sub=train_gen.input_transform.sub,\n",
    "        norm_q=scale_dict['PHQ'],\n",
    "        hyai=hyai, hybi=hybi\n",
    "    )([inpC, massout])\n",
    "    NNA = tf.keras.models.Model(inpC, enthout)\n",
    "    print(NNA.summary())\n",
    "\n",
    "    # 2) Compile model\n",
    "    NNA.compile(tf.keras.optimizers.RMSprop(), loss=mse, metrics=[mse])\n",
    "\n",
    "    # 3) Train model\n",
    "    NNA.fit_generator(train_gen, epochs=Nep, validation_data=valid_gen)\n",
    "\n",
    "    # 4) Save model\n",
    "    path = TRAINDIR+'HDF5_DATA/MLRA.h5'\n",
    "    NNA.save(path)\n",
    "    print('MLR saved in ',path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Calculate statistics and residuals in a loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1) Load models to test statistics\n",
    "tgb - 4/19/2019 - Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_fn = '/filer/z-sv-pool12c/t/Tom.Beucler/SPCAM/CBRAIN-CAM/pp_config/8col_rad_tbeucler_local_PostProc.yml'\n",
    "# data_fn = '/local/Tom.Beucler/SPCAM_PHYS/8col009_01_valid.nc'\n",
    "# dict_lay = {'SurRadLayer':SurRadLayer,'MassConsLayer':MassConsLayer,'EntConsLayer':EntConsLayer}\n",
    "\n",
    "# %cd $TRAINDIR/HDF5_DATA\n",
    "# !ls\n",
    "# dict_lay = {'SurRadLayer':SurRadLayer,'MassConsLayer':MassConsLayer,'EntConsLayer':EntConsLayer}\n",
    "# alpha = 0\n",
    "# path = TRAINDIR+'HDF5_DATA/NNL'+str(alpha)+'.h5'\n",
    "# NN = load_model(path,custom_objects=dict_lay)\n",
    "# md = ModelDiagnostics(NN,config_fn,data_fn)\n",
    "    \n",
    "# # 3) Calculate statistics and save in pickle file\n",
    "# md.compute_stats(niter=5)\n",
    "# path = TRAINDIR+'HDF5_DATA/NNL'+str(alpha)+'md.pkl'\n",
    "# pickle.dump(md.stats,open(path,'wb'))\n",
    "# print('Stats are saved in ',path)\n",
    "\n",
    "# # 4) Calculate budget residuals and save in pickle file\n",
    "# md.compute_res(niter=5)\n",
    "# path = TRAINDIR+'HDF5_DATA/NNL'+str(alpha)+'res.pkl'\n",
    "# pickle.dump(md.res,open(path,'wb'))\n",
    "# print('Budget residuals are saved in ',path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2) Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_fn = '/filer/z-sv-pool12c/t/Tom.Beucler/SPCAM/CBRAIN-CAM/pp_config/8col_rad_tbeucler_local_PostProc.yml'\n",
    "# data_fn = '/local/Tom.Beucler/SPCAM_PHYS/8col009_01_valid.nc'\n",
    "# dict_lay = {'SurRadLayer':SurRadLayer,'MassConsLayer':MassConsLayer,'EntConsLayer':EntConsLayer}\n",
    "\n",
    "# for alpha in alpha_array:\n",
    "    \n",
    "#     # 1) Load model\n",
    "#     path = TRAINDIR+'HDF5_DATA/NNL'+str(alpha)+'.h5'\n",
    "#     NN = load_model(path,custom_objects=dict_lay)\n",
    "    \n",
    "#     # 2) Define model diagnostics object\n",
    "#     md = ModelDiagnostics(NN,config_fn,data_fn)\n",
    "    \n",
    "#     # 3) Calculate statistics and save in pickle file\n",
    "#     md.compute_stats()\n",
    "#     path = TRAINDIR+'HDF5_DATA/NNL'+str(alpha)+'md.pkl'\n",
    "#     pickle.dump(md.stats,open(path,'wb'))\n",
    "#     print('Stats are saved in ',path)\n",
    "    \n",
    "#     # 4) Calculate budget residuals and save in pickle file\n",
    "#     md.compute_res()\n",
    "#     path = TRAINDIR+'HDF5_DATA/NNL'+str(alpha)+'res.pkl'\n",
    "#     pickle.dump(md.res,open(path,'wb'))\n",
    "#     print('Budget residuals are saved in ',path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics of architecture-constrained neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/t/Tom.Beucler/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/t/Tom.Beucler/miniconda3/lib/python3.6/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/t/Tom.Beucler/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f12cacfafa4ea7853b6cef91283ab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5172), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# config_fn = '/filer/z-sv-pool12c/t/Tom.Beucler/SPCAM/CBRAIN-CAM/pp_config/8col_rad_tbeucler_local_PostProc.yml'\n",
    "# data_fn = '/local/Tom.Beucler/SPCAM_PHYS/8col009_01_valid.nc'\n",
    "# dict_lay = {'SurRadLayer':SurRadLayer,'MassConsLayer':MassConsLayer,'EntConsLayer':EntConsLayer}\n",
    "\n",
    "# NN = {}; MD = {};\n",
    "# # 1) Load model\n",
    "# path = TRAINDIR+'HDF5_DATA/NNA.h5'\n",
    "# NN = load_model(path,custom_objects=dict_lay)\n",
    "\n",
    "# # 2) Define model diagnostics object\n",
    "# md = ModelDiagnostics(NN,config_fn,data_fn)\n",
    "\n",
    "# # 3) Calculate statistics and save in pickle file\n",
    "# # md.compute_stats()\n",
    "# # path = TRAINDIR+'HDF5_DATA/NNAmd.pkl'\n",
    "# # pickle.dump(md.stats,open(path,'wb'))\n",
    "# # print('Stats are saved in ',path)\n",
    "\n",
    "# # 4) Calculate budget residuals and save in pickle file\n",
    "# md.compute_res()\n",
    "# path = TRAINDIR+'HDF5_DATA/NNAres.pkl'\n",
    "# pickle.dump(md.res,open(path,'wb'))\n",
    "# print('Budget residuals are saved in ',path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics of multiple linear regressions  \n",
    "tgb - 4/23/2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/t/Tom.Beucler/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/t/Tom.Beucler/miniconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e9acc87ecf4b36b103f82255d211c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5172), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Budget residuals are saved in  /local/Tom.Beucler/SPCAM_PHYS/HDF5_DATA/MLRL0res.pkl\n",
      "WARNING:tensorflow:From /home/t/Tom.Beucler/miniconda3/lib/python3.6/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91825877a252499db55860cce5118e1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5172), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/filer/z-sv-pool12c/t/Tom.Beucler/SPCAM/CBRAIN-CAM/notebooks/tbeucler_devlog/cbrain/model_diagnostics.py:143: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  self.stats['r2'] = 1. - (self.stats['mse'] / self.stats['true_var'])\n",
      "/filer/z-sv-pool12c/t/Tom.Beucler/SPCAM/CBRAIN-CAM/notebooks/tbeucler_devlog/cbrain/model_diagnostics.py:149: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  self.stats['hor_r2'] = 1 - (self.stats['hor_mse'] / self.stats['hor_tvar'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats are saved in  /local/Tom.Beucler/SPCAM_PHYS/HDF5_DATA/MLRAmd.pkl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107102fe87534ac98f24a81a79596a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5172), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Budget residuals are saved in  /local/Tom.Beucler/SPCAM_PHYS/HDF5_DATA/MLRAres.pkl\n"
     ]
    }
   ],
   "source": [
    "config_fn = '/filer/z-sv-pool12c/t/Tom.Beucler/SPCAM/CBRAIN-CAM/pp_config/8col_rad_tbeucler_local_PostProc.yml'\n",
    "data_fn = '/local/Tom.Beucler/SPCAM_PHYS/8col009_01_valid.nc'\n",
    "dict_lay = {'SurRadLayer':SurRadLayer,'MassConsLayer':MassConsLayer,'EntConsLayer':EntConsLayer}\n",
    "\n",
    "# 1) Load model\n",
    "path = TRAINDIR+'HDF5_DATA/MLRL0.h5'\n",
    "NN = load_model(path,custom_objects=dict_lay)\n",
    "\n",
    "# 2) Define model diagnostics object\n",
    "md = ModelDiagnostics(NN,config_fn,data_fn)\n",
    "\n",
    "# 3) Calculate statistics and save in pickle file\n",
    "# md.compute_stats()\n",
    "# path = TRAINDIR+'HDF5_DATA/MLRL0md.pkl'\n",
    "# pickle.dump(md.stats,open(path,'wb'))\n",
    "# print('Stats are saved in ',path)\n",
    "\n",
    "# 4) Calculate budget residuals and save in pickle file\n",
    "md.compute_res()\n",
    "path = TRAINDIR+'HDF5_DATA/MLRL0res.pkl'\n",
    "pickle.dump(md.res,open(path,'wb'))\n",
    "print('Budget residuals are saved in ',path)\n",
    "\n",
    "NN = {}; md = {};\n",
    "\n",
    "# 1) Load model\n",
    "path = TRAINDIR+'HDF5_DATA/MLRA.h5'\n",
    "NN = load_model(path,custom_objects=dict_lay)\n",
    "\n",
    "# 2) Define model diagnostics object\n",
    "md = ModelDiagnostics(NN,config_fn,data_fn)\n",
    "\n",
    "# 3) Calculate statistics and save in pickle file\n",
    "md.compute_stats()\n",
    "path = TRAINDIR+'HDF5_DATA/MLRAmd.pkl'\n",
    "pickle.dump(md.stats,open(path,'wb'))\n",
    "print('Stats are saved in ',path)\n",
    "\n",
    "# 4) Calculate budget residuals and save in pickle file\n",
    "md.compute_res()\n",
    "path = TRAINDIR+'HDF5_DATA/MLRAres.pkl'\n",
    "pickle.dump(md.res,open(path,'wb'))\n",
    "print('Budget residuals are saved in ',path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
